\chapter{Background}\label{chapter:background}

This chapter gives an overview of prerequisites upon which we build our
cross-chain protocols.

\section{Notation}
We use standard mathematical notation throughout this work. We define all the
non-standard or unusual notation in this section. We use standard cryptographic
notation, which is also introduced here for reference. The reader unfamiliar
with this notation can consult some reference book in the subject such
as~\cite{katz} for a more complete treatment.

Given a distribution $\mathcal{M}$, we denote by $m \gets \mathcal{M}$ the experiment by which the random variable $m$ is chosen according to the distribution $\mathcal{M}$. Given a finite set $M$, we use $\uniform(M)$ to denote the uniform distribution which assigns probability $1 / 2^{|M|}$ to each element $m \in M$. We will use $m \getsrandomly M$ to denote the experiment in which $m$ is sampled from $\uniform(M)$.

As a shorthand for probabilities and to avoid excessive subscripting, we will write the experimental set up (such as the sampling of random variables) within the $\Pr[\cdot]$ prior to the predicate of interest and separated by $;\,$. For example, $\Pr[x \gets \mathcal{D}_1, y \gets \mathcal{D}_2; x + y = 1]$ denotes the experiment of independently sampling two random variables, $x$ and $y$, from the distributions $\mathcal{D}_1$ and $\mathcal{D}_2$ respectively, summing their values, and observing whether their sum is equal to $1$.

\subsection{Asymptotic probabilistic security}
Following the extended Church--Turing definition, we consider the class of
problems in $\textsc{P}$ to be \emph{easy}, and we call \emph{hard} those which
are not easy~\cite{sipser}. It is assumed that all honest parties and the adversary have polynomial available time. Both the honest parties and the adversary have access to true randomness and are thus probabilistic Turing Machines.
The shorthand \emph{PPT} is used to denote a probabilistic polynomial-time
Turing machine.

Our theorems are by computational reduction, in which we show that bad
events happen only with negligible probability in some security parameter, which
we will denote $\lambda \in \mathbb{N}$. This parameter allows all our
cryptographic primitives to be instantiated with the required level of security;
for example, it provides the number of bits in the output of our hash function.

\begin{definition}[Negligible]
  A function $f: \mathbb{N} \longrightarrow \mathbb{R}^+$ is
  \emph{negligible} if for all $k \in \mathbb{N}$ it holds that
  $f \in \mathcal{O}(\frac{1}{\lambda^k})$.
\end{definition}

We will use the notation $\negl$ to denote any negligible function.

\begin{definition}[Overwhelming]
  A function $f: \mathbb{N} \longrightarrow \mathbb{R}^+$ is
  \emph{overwhelming} if it can be written as $f(n) = 1 - \negl(n)$ for
  some negligible function $\negl$.
\end{definition}

We will define the \emph{security} of various cryptographic protocols by making use of challenger-adversary games in which the challenger is a known Turing Machine defined by us, but the adversary is an \emph{arbitrary} Turing Machine. Herein lies the beauty of cryptography as a science; it allows us to create protocols which we prove secure against \emph{any} adversary, even those we cannot conceive. A protocol will be considered \emph{secure} if no PPT adversary can win the respective game, except with negligible probability.

\subsection{Sequences}
We use $[n]$ to denote the set of natural numbers from $0$ up to and including
$n$. We also use $[\mathcal{M}]$ to denote the support of a distribution
$\mathcal{M}$; the distinction between the two notations will be clear from
context.
We use $\epsilon$ to denote the empty sequence (or empty string). We write
one sequence next to another to denote string concatenation. Likewise, we
concatenate sequences to symbols by juxtaposition.

Our sequences are indexed starting at $0$. Given a sequence $\chain$, we use
$|\chain|$ to denote its length. We use Python notation to denote sequence
addressing. For $i \in [n - 1]$, we denote the $i^\text{th}$ element from the
beginning as $\chain[i]$. The first element of the sequence is thus $\chain[0]$.
For $i \in [n] \setminus \{0\}$, we denote the $i^\text{th}$ element from the
end as $\chain[-i]$. The last element of the sequence is thus $\chain[-1]$. We
call this the \emph{tip} of the sequence. Given $i \in \mathbb{Z},
j \in \mathbb{Z}$ with $i \leq j$, we denote $\chain[i{:}j]$ the subsequence from
$i$ (inclusive) to $j$ (exclusive), that is the sequence which contains exactly
the elements $\chain[i], \chain[i + 1], \dots, \chain[j - 1]$. If $i > j$, then
by convention we set $\chain[i{:}j] = \epsilon$. We allow this \emph{range}
notation to be used with negative indices as well, indicating ranges starting or
ending (or both) in indices considered from the end of the sequence, hence
allowing for $\chain[-i{:}j], \chain[i{:}-j]$, and $\chain[-i{:}-j]$. The left end of
a range can omitted if it is $i = 0$. The right end of a range can be omitted if
it is $j = |\chain|$. For example, $\chain[:-k]$ is the sequence $\chain$ with
the last $k$ elements excluded. In this example, if $|\chain| < k$, then
$\chain[:-k] = \epsilon$.

If an element $A$ is a member of a sequence $\chain$ we will use the notation $A
\in \chain$ to denote this, i.e. that there exist words $w, v$ such that $\chain
= wAv$. It will be clear from the context whether we are speaking about sequence
or sets. Given $A, Z \in \chain$ such that $A$ and $Z$ exist only once in
$\chain$, we denote by $\chain\{A{:}Z\}$ the subsequence of $\chain$ starting from
$A$ (inclusive) and ending in $Z$ (exclusive). If $A = \chain[0]$, it can be
omitted. Omitting $Z$ denotes the sequence starting with $A$ and containing all
subsequent elements until the end of the sequence.

% TODO: \chain\{A:Z\} notation (inclusive, exclusive). But how to include Z?
\section{Cryptographic Primitives}

We now overview the cryptographic primitives we will make use of. In particular,
cryptographically secure hash functions, public-key signatures, and
proof-of-work. Notably, similar to most blockchain protocols, we will not be
using any encryption or decryption. This section is a review. For a full
treatment, refer to any introductory textbook in the subject~\cite{katz,handbook,foundations1,foundations2}.

\subsection{Hash Functions}
A hash function $H^s: \mathcal{M} \longrightarrow \{0, 1\}^\lambda$ is a function
parameterized by the security parameter $\lambda$ which takes any string from the distribution of input strings $\mathcal{M}$ and
outputs a string of constant size $\lambda$. To capture the fact that the hash
function behaves like a randomly chosen function, the hash
function is instantiated using a key-generating function
$\textsf{Gen}(1^\lambda)$ which generates a hash key $s$. The hash function itself is then $H^s$, a different function for each value of the key $s$. As hash functions are the building blocks and workhoses of cryptography, other protocols are designed on top of them that make use of them. We will do so in this work. In practice, the key $s$ is assumed to have been generated by the designers of the higher level protocol that makes use of the hash function and is typically fixed and publicly known. The hash protocol is the tuple $\Pi = (\textsf{Gen}, H)$.

Practical hash functions allow us to map any message $x$ of arbitrary length
$x \in \{0, 1\}^*$ to a fixed-length bitstring $\{0, 1\}^\lambda$. Hash
functions are easy to compute, but hard to invert. In applications, it is
assumed that a hash uniquely represents its preimage (it is \emph{binding}) and
that the preimage cannot be discovered from the image given sufficient entropy
(it is \emph{hiding}).

These intuitive ideas are captured by the difficulty of finding
collisions in hash functions. This is formalized in the next definition.

\input{./chapters/background/algorithms/alg.collision-resistance.tex}

\begin{definition}[Collision resistance]
  A hash function $H: \{0, 1\}^* \longrightarrow \{0, 1\}^\lambda$ is called
  \emph{collision resistant} if for all PPT adversaries $\mathcal{A}$ there is a
  negligible function $\negl$ such that

  \[
  \Pr[\textsf{hash-collision}_{\Pi,\mathcal{A}} = 1] \leq \negl(\lambda)\,.
  \]
\end{definition}

A weaker notion of security mandates that no adversary can reverse the function (pre-image resistance) or that no adversary can find a second value giving the same output as a given random value. The two cryptographic games and definitions are illustrated in Algorithms~\ref{alg.hash-preimage} and~\ref{alg.hash-second-preimage}.

\input{./chapters/background/algorithms/alg.hash-preimage.tex}
\input{./chapters/background/algorithms/alg.hash-second-preimage.tex}

\begin{definition}[Pre-image resistance]
  A hash function $H: \{0, 1\}^* \longrightarrow \{0, 1\}^\lambda$ is called
  \emph{pre-image resistant} if for all PPT adversaries $\mathcal{A}$ there is a
  negligible function $\negl$ such that

  \[
  \Pr[\textsf{hash-preimage}_{\Pi,\mathcal{A}} = 1] \leq \negl(\lambda)\,.
  \]
\end{definition}

\begin{definition}[Second pre-image resistance]
  A hash function $H: \{0, 1\}^* \longrightarrow \{0, 1\}^\lambda$ is called
  \emph{second pre-image resistant} if for all PPT adversaries $\mathcal{A}$
  there is a negligible function $\negl$ such that

  \[
  \Pr[\textsf{hash-second-preimage}_{\Pi,\mathcal{A}} = 1] \leq \negl(\lambda)\,.
  \]
\end{definition}

A hash function that is collision-resistant is also pre-image resistant; additionally, if it is pre-image resistant, then it must also be second pre-image resistant, as long as it provides sufficient compression~\cite{rogaway2004cryptographic}.

Protocols deployed in practice make use of fixed hash functions; that is, hash
functions with a fixed security parameter and a fixed key. In Bitcoin, the hash
function $\SHA$~\cite{sha256} is used for both commitments and proof-of-work.
Its domain and range are
$\SHA: \{0, 1\}^* \longrightarrow \{0, 1\}^{256}$. In Ethereum, the hash
function $\keccak$~\cite{bertoni2008indifferentiability}, a variant of
$\texttt{SHA3}$, is used for commitments. Its domain and range are $\keccak: \{0, 1\}^*
\longrightarrow \{0, 1\}^{256}$. The function used for proof-of-work is a
variant of this.

\subsection{Signatures}
A \emph{digital signature} allows parties to authenticate the origin of a
message as well as its integrity. If Alice signs a message $m$, she generates a
signature $\sigma$ which is uniquely associated with that message. That
signature can then only be used to verify that particular message. An adversary
cannot \emph{forge} signatures that correctly verify for messages that have not
been signed by the honest party.

Signing and verification are two separate tasks which are asymmetric. Only the
authorized party can sign a message, but anyone can verify the signature. This
is achieved by having each party generate their own \emph{public-private
key pair} $(pk, sk)$ in which $pk$ is the public key and $sk$ is the secret key.
Signatures are then generated using the secret (or signing) key $sk$ and
verified usin the public (or verification) key $sk$. A key pair is generated
using the polynomial-time key generation algorithm $(pk, sk) \gets
\Gen(1^\lambda)$. A signature is generated by invoking the polynomial-time
signing algorithm $\sigma = \Sig(sk, m)$. Verification is done by checking
whether the verification algorithm $\Ver(pk, m, \sigma)$ returns $\true$ or
$\false$. The signature scheme $\Pi$ then is defined as the tuple
$\Pi = (\Gen, \Sig, \Ver)$.

Signature schemes must be \emph{correct}.

\begin{definition}[Signature correctness]
  A signature scheme is \emph{correct} if there is a
  negligible function $\negl$ such that for all messages $m \in \{0, 1\}^*$ it
  holds that

  \[
    \Pr[(pk, sk) \gets \Gen(1^\lambda); \Ver(pk, m, \Sig(sk, m)) = \false] < \negl(\lambda)\,.
  \]
\end{definition}

A \emph{secure} signature scheme requires that no adversary is able to forge
signatures. This is captured in the game-based definition of Algorithm~\ref{alg.forgery}.

\input{./chapters/background/algorithms/alg.forgery.tex}

% elliptic curves, secp256k1
\subsection{Proof-of-Work}

\section{The Transaction Layer}
\subsection{Transactions}
\subsection{The UTXO Model}
\subsection{The Account Model}

\section{Authenticated Data Structures}
\subsection{Merkle Trees}
\subsection{Merkle–Patricia Tries}

\section{Blockchains}
\subsection{The Consensus Problem}
\subsection{Blocks}
\subsection{Chains of Blocks}
\subsection{Blockchain Addressing}
\subsection{SPV}

\section{Cryptocurrencies}
\subsection{Bitcoin}
\subsection{Ethereum}

\section{Smart Contracts}
\subsection{Bitcoin Script}
\subsection{Solidity}

\section{Mathematical Background}
\subsection{The Chernoff Bound}

The following well-known theorem is due to
Rubin~\cite{chernoff1952measure,chernoff2014career}. It will help us derive
negligible bounds for probabilities of bad events.

\begin{theorem}[Chernoff Bounds]
  Let $\{X_i: i \in [n]\}$ be mutually independent Boolean random variables
  such that for all $i \in [n]$ it holds that $\Pr[X_i = 1] = p$. Let
  $X = \sum_{i = 1}^n X_i$ and $\mu = \Ex[X] = pn$. Then, for all
  $\delta \in (0, 1]$ it holds that:

  \[
    \Pr[X \leq (1 - \delta)\mu] \leq \exp(-\frac{\delta^2\mu}{2})
    \text{ and }
    \Pr[X \geq (1 + \delta)\mu] \leq \exp(-\frac{\delta^2\mu}{3})
  \]
\end{theorem}

\section{Model}
\subsection{The Random Oracle}
% Random Oracles as machines and as random functions
\subsection{The Synchronous and Bounded Delay Setting}
\subsection{Blockchain Backbone}
% Honest Majority Assumption
% Chain Growth
% Common Prefix
% Chain Quality
% The Constant Difficulty assumption and its relaxation
\subsection{The Common Reference String}
% Mention that CRS is not needed (due to Bootstrapping the Blockchain - Directly paper)
\subsection{Ouroboros}
% Epochs
